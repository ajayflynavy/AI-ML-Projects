{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Using Bert for Sentiment Analysis.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "RDkWa9cZ8cpQ",
        "RVwj1wHk8-jR",
        "yFrQuGJRPNV3",
        "tyeXdwsX-6Pi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-joZUrKBxZlp"
      },
      "source": [
        "### Install Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbf8I6C3xRpu"
      },
      "source": [
        "!pip3 install --quiet transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xLjTD2tA68E"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK1hCNx3ybHG"
      },
      "source": [
        "### Download Data From Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Pi9Yhtyc9-"
      },
      "source": [
        "!pip3 install kaggle --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYVKufdazNny"
      },
      "source": [
        "#Make a directory for Kaggle\n",
        "!mkdir .kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TU0n8jozRjl"
      },
      "source": [
        "#Connect Google drive to colab\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ-7aJD8zTxt"
      },
      "source": [
        "#Copy kaggle.json file\n",
        "!cp '/gdrive/My Drive/AI-ML/Machine-Learning/Code/Utilities/kaggle.json' /content/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOhD_EowzdBF"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "!kaggle config set -n path -v{/content}\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AnafDsazof9"
      },
      "source": [
        "Verify Kaggle connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAh_zhqYznqJ"
      },
      "source": [
        "!kaggle datasets list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaECNGNyz1tV"
      },
      "source": [
        "#Download Movie Reviews data\n",
        "!kaggle competitions download -c word2vec-nlp-tutorial -p /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrC8pW-i07xM"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R5wUDm20_q9"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHhezTdp6ZNc"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRKLhEYkyd8h"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk4bGHTR1Ba2"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY2nkFUJ1JbG"
      },
      "source": [
        "df = pd.read_csv('./labeledTrainData.tsv.zip', sep='\\t')\n",
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_jvGnes1TQt"
      },
      "source": [
        "df.sample(n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFKnF30c2hHs"
      },
      "source": [
        "#Sentences and labels\n",
        "sentences = df.review.values\n",
        "labels = df.sentiment.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VFZi7TZ6blJ"
      },
      "source": [
        "Tokenize data using Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAV1wcHf3Mri"
      },
      "source": [
        "from transformers import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xca4ytQQ2o_x"
      },
      "source": [
        "#Get BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP4OJIP04q_K"
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmXy2EiijmU1"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y55BhKZW3GHf"
      },
      "source": [
        "#Check tokenized text\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCyLM9jL5SE2"
      },
      "source": [
        "#We will use only first 200 tokens to do classification (this value can be changed)\n",
        "max_length = 200\n",
        "tokenized_texts = [sent[:max_length] for sent in tokenized_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dM2OE85C2uU"
      },
      "source": [
        "for i in range(len(tokenized_texts)):\n",
        "    sent = tokenized_texts[i]\n",
        "    sent = ['[CLS]'] + sent + ['[SEP]']\n",
        "    tokenized_texts[i] = sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP7qJPVYEZk0"
      },
      "source": [
        "print(tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgFWkwEf3_MY"
      },
      "source": [
        "#Convert tokens into IDs\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4Slg7b84Sd7"
      },
      "source": [
        "print(input_ids[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKsOey_557B_"
      },
      "source": [
        "#Pad our tokens which might be less than max_length size\n",
        "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=max_length+2, truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZdKrRK_68CK"
      },
      "source": [
        "Split data between training and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb-rtcT-6NbK"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc8O7AKC7CQ4"
      },
      "source": [
        "#80% data will be used for training while 20% will be used for test\n",
        "trainX, testX, trainY, testY = train_test_split(input_ids, labels, test_size=0.2, random_state=12345)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtJhkg4f7XBA"
      },
      "source": [
        "Create Attention masks : Attention masks are useful to ignore padding tokens. Mask value will be set to 0 for padding tokens and 1 for actual tokens. We will create mask both for training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LATbNgo87VwS"
      },
      "source": [
        "# Create attention masks for training\n",
        "train_attn_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in trainX:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  train_attn_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inz335am8AQq"
      },
      "source": [
        "# Create attention masks for Test\n",
        "test_attn_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in testX:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  test_attn_masks.append(seq_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dcq2O3ZN8XRr"
      },
      "source": [
        "At this point, we have the data ready"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDkWa9cZ8cpQ"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNcElQY48InX"
      },
      "source": [
        "#Load Pre-trained Bert Model with a Binary Classification layer at the top.\n",
        "#Huggingface library provides TFBertForSequenceClassification for the same\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUoCTeRa-JyO"
      },
      "source": [
        "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XqXIguX8oMa"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVwj1wHk8-jR"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chuCqqJw_8cc"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWtlDD6PAYNP"
      },
      "source": [
        "train_x_data = {'input_ids': np.array(trainX), 'attention_mask': np.array(train_attn_masks)}\n",
        "test_x_data = {'input_ids': np.array(testX), 'attention_mask': np.array(test_attn_masks)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ya5DYBlD9AUx"
      },
      "source": [
        "model.fit(train_x_data, trainY, validation_data=(test_x_data, testY), batch_size=16, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}